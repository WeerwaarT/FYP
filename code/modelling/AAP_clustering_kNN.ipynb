{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-14T19:16:04.090415600Z",
     "start_time": "2023-05-14T19:15:59.944578900Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "thresholds = [10, 20, 50, 100, 200]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-14T19:16:04.106429700Z",
     "start_time": "2023-05-14T19:16:04.092416900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "AAP = {}\n",
    "AAP_normalized = {}\n",
    "for t in thresholds:\n",
    "    df = pd.read_csv(f'../LFM-1b_UGP/AAP/LFM-1b_AAP_{t}.txt', sep='\\t')\n",
    "    df.set_index('artist_id', inplace=True)\n",
    "    df = df.loc[(df.sum(axis=1) != 0)]\n",
    "    index_to_artist_id = {index: artist_id for index, artist_id in enumerate(df.index.tolist())}\n",
    "    artist_id_to_index = {artist_id: index for index, artist_id in enumerate(df.index.tolist())}\n",
    "    AAP[t] = (df, index_to_artist_id, artist_id_to_index)\n",
    "\n",
    "    df = pd.read_csv(f'../LFM-1b_UGP/AAP/LFM-1b_AAP_{t}_normalized.txt', sep='\\t')\n",
    "    df.set_index('artist_id', inplace=True)\n",
    "    df = df.loc[(df.sum(axis=1) != 0)]\n",
    "    index_to_artist_id = {index: artist_id for index, artist_id in enumerate(df.index.tolist())}\n",
    "    artist_id_to_index = {artist_id: index for index, artist_id in enumerate(df.index.tolist())}\n",
    "    AAP_normalized[t] = (df, index_to_artist_id, artist_id_to_index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-14T19:16:07.300338200Z",
     "start_time": "2023-05-14T19:16:04.110433200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import heapq\n",
    "import skfuzzy as fuzz\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-14T19:16:09.607958Z",
     "start_time": "2023-05-14T19:16:07.302340Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "df_plays_test = pd.read_csv('../testing/test_playcounts.txt', sep='\\t')\n",
    "user_ids = df_plays_test['user_id'].unique()\n",
    "liked_artists = {}\n",
    "for t in thresholds:\n",
    "    mask = df_plays_test['artist_id'].isin(AAP[t][2]) & (df_plays_test['playcount'] >= t)\n",
    "    liked_artists[t] = df_plays_test[mask].groupby('user_id')['artist_id'].apply(list).to_dict(into=defaultdict(list))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-14T19:16:09.779114Z",
     "start_time": "2023-05-14T19:16:09.609959900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def train_and_test(threshold: int, normalized: bool, n_clusters: int, m_value: float = 2.0):\n",
    "    if normalized:\n",
    "        data = AAP[threshold]\n",
    "    else:\n",
    "        data = AAP_normalized[threshold]\n",
    "\n",
    "    _, u, _, _, jm, _, fpc = fuzz.cmeans(data[0].values.T, n_clusters, m_value, error=0.005, maxiter=2000)\n",
    "    if len(jm) == 2000 and abs(jm[-2] - jm[-1]) > 0.005:\n",
    "        print(f\"The algorithm didn't converge.  {m_value} {n_clusters}\")\n",
    "\n",
    "    return u.T"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-14T19:16:09.795128600Z",
     "start_time": "2023-05-14T19:16:09.782116700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def recommend_and_evaluate(fuzzy_c_partitioned_matrix, threshold: int, normalized: bool):\n",
    "    if normalized:\n",
    "        data = AAP[threshold]\n",
    "    else:\n",
    "        data = AAP_normalized[threshold]\n",
    "\n",
    "    precisions = []\n",
    "    distances = []\n",
    "    for i, user_id in enumerate(user_ids):\n",
    "        # print(f\"User {i}\")\n",
    "        artist_ids = liked_artists[threshold][user_id]\n",
    "        if len(artist_ids) < 2:\n",
    "            continue\n",
    "\n",
    "        if len(artist_ids) < 5:\n",
    "            train_artists, test_artists = train_test_split(artist_ids, train_size=0.75, random_state=42)\n",
    "        else:\n",
    "            train_artists, test_artists = train_test_split(artist_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "        k = 100\n",
    "        top_k = []\n",
    "        for artist_id in train_artists:\n",
    "            index = data[2][artist_id]\n",
    "            pw_distances = pairwise_distances(fuzzy_c_partitioned_matrix[[index]], fuzzy_c_partitioned_matrix)\n",
    "            pw_distances[0][index] = float('inf')\n",
    "            for idx, distance in enumerate(pw_distances[0]):\n",
    "                # if sim_value > 0.8:\n",
    "                #     continue\n",
    "\n",
    "                if len(top_k) < k:\n",
    "                    heapq.heappush(top_k, (distance, idx))\n",
    "                else:\n",
    "                    if distance < top_k[0][0]:\n",
    "                        heapq.heapreplace(top_k, (distance, idx))\n",
    "\n",
    "        positive = 0\n",
    "        for distance, index in top_k:\n",
    "            artist_id = data[1][index]\n",
    "            if artist_id in test_artists:\n",
    "                # print(f\"GOAL!       {distance}\")\n",
    "                positive += 1\n",
    "                distances.append(distance)\n",
    "\n",
    "        precisions.append(positive / min(k, len(test_artists)))\n",
    "\n",
    "    print(f'precision: {sum(precisions) / len(precisions)}')\n",
    "    # print('distances')\n",
    "    # print(distances)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-14T19:16:09.815146600Z",
     "start_time": "2023-05-14T19:16:09.801133800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "models = defaultdict(dict)\n",
    "models_normalized = defaultdict(dict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-14T19:16:09.828158300Z",
     "start_time": "2023-05-14T19:16:09.811143200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold:  10 n_clusters: 10\n",
      "precision: 0.042934975141200864\n",
      "threshold:  10 n_clusters: 10    normalized\n",
      "precision: 0.04340409315130248\n",
      "--------------------------------------------------------------\n",
      "threshold:  20 n_clusters: 10\n",
      "precision: 0.046227113240847675\n",
      "threshold:  20 n_clusters: 10    normalized\n",
      "precision: 0.046116633009549324\n",
      "--------------------------------------------------------------\n",
      "threshold:  50 n_clusters: 10\n",
      "precision: 0.052501717245090083\n",
      "threshold:  50 n_clusters: 10    normalized\n",
      "precision: 0.05198934034863014\n",
      "--------------------------------------------------------------\n",
      "threshold:  100 n_clusters: 10\n",
      "precision: 0.0537843175609133\n",
      "threshold:  100 n_clusters: 10    normalized\n",
      "precision: 0.05359089396323438\n",
      "--------------------------------------------------------------\n",
      "threshold:  200 n_clusters: 10\n",
      "precision: 0.06699503148366785\n",
      "threshold:  200 n_clusters: 10    normalized\n",
      "precision: 0.06813139512003148\n",
      "--------------------------------------------------------------\n",
      "threshold:  10 n_clusters: 15\n",
      "precision: 0.04363654099123563\n",
      "threshold:  10 n_clusters: 15    normalized\n",
      "precision: 0.04341016479671839\n",
      "--------------------------------------------------------------\n",
      "threshold:  20 n_clusters: 15\n",
      "precision: 0.046433554198733726\n",
      "threshold:  20 n_clusters: 15    normalized\n",
      "precision: 0.046116633009549324\n",
      "--------------------------------------------------------------\n",
      "threshold:  50 n_clusters: 15\n",
      "precision: 0.049115445692728577\n",
      "threshold:  50 n_clusters: 15    normalized\n",
      "precision: 0.05278573733879876\n",
      "--------------------------------------------------------------\n",
      "threshold:  100 n_clusters: 15\n",
      "precision: 0.05425713079968399\n",
      "threshold:  100 n_clusters: 15    normalized\n",
      "precision: 0.0537843175609133\n",
      "--------------------------------------------------------------\n",
      "threshold:  200 n_clusters: 15\n",
      "precision: 0.06813139512003148\n",
      "threshold:  200 n_clusters: 15    normalized\n",
      "precision: 0.06623745572609209\n",
      "--------------------------------------------------------------\n",
      "threshold:  10 n_clusters: 20\n",
      "precision: 0.04336685645779352\n",
      "threshold:  10 n_clusters: 20    normalized\n",
      "precision: 0.04381089339416829\n",
      "--------------------------------------------------------------\n",
      "threshold:  20 n_clusters: 20\n",
      "precision: 0.04638124336777617\n",
      "threshold:  20 n_clusters: 20    normalized\n",
      "precision: 0.046116633009549324\n",
      "--------------------------------------------------------------\n",
      "threshold:  50 n_clusters: 20\n",
      "precision: 0.04906052176237211\n",
      "threshold:  50 n_clusters: 20    normalized\n",
      "precision: 0.052912062378618605\n",
      "--------------------------------------------------------------\n",
      "threshold:  100 n_clusters: 20\n",
      "precision: 0.05354791094152796\n",
      "threshold:  100 n_clusters: 20    normalized\n",
      "precision: 0.05359089396323438\n",
      "--------------------------------------------------------------\n",
      "threshold:  200 n_clusters: 20\n",
      "precision: 0.06813139512003148\n",
      "threshold:  200 n_clusters: 20    normalized\n",
      "precision: 0.06813139512003148\n",
      "--------------------------------------------------------------\n",
      "threshold:  10 n_clusters: 25\n",
      "precision: 0.04354833615349693\n",
      "threshold:  10 n_clusters: 25    normalized\n",
      "precision: 0.04366312151700367\n",
      "--------------------------------------------------------------\n",
      "threshold:  20 n_clusters: 25\n",
      "precision: 0.04636271661514538\n",
      "threshold:  20 n_clusters: 25    normalized\n",
      "precision: 0.046242292723045184\n",
      "--------------------------------------------------------------\n",
      "threshold:  50 n_clusters: 25\n",
      "precision: 0.051037783255204534\n",
      "threshold:  50 n_clusters: 25    normalized\n",
      "precision: 0.0533445883301757\n",
      "--------------------------------------------------------------\n",
      "threshold:  100 n_clusters: 25\n",
      "precision: 0.05354791094152796\n",
      "threshold:  100 n_clusters: 25    normalized\n",
      "precision: 0.0537843175609133\n",
      "--------------------------------------------------------------\n",
      "threshold:  200 n_clusters: 25\n",
      "precision: 0.06813139512003148\n",
      "threshold:  200 n_clusters: 25    normalized\n",
      "precision: 0.06813139512003148\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for n_clusters in [10, 15, 20, 25]:\n",
    "    for t in thresholds:\n",
    "        print(f\"threshold:  {t} n_clusters: {n_clusters}\")\n",
    "        if t in models and n_clusters in models[t]:\n",
    "            model = models[t][n_clusters]\n",
    "            print(\"Found\")\n",
    "        else:\n",
    "            model = train_and_test(t, False, n_clusters)\n",
    "            models[t][n_clusters] = model\n",
    "\n",
    "        recommend_and_evaluate(model, t, False)\n",
    "\n",
    "        print(f\"threshold:  {t} n_clusters: {n_clusters}    normalized\")\n",
    "        if t in models_normalized and n_clusters in models_normalized[t]:\n",
    "            model_normalized = models_normalized[t][n_clusters]\n",
    "            print(\"Found\")\n",
    "        else:\n",
    "            model_normalized = train_and_test(t, True, n_clusters)\n",
    "            models_normalized[t][n_clusters] = model_normalized\n",
    "\n",
    "        recommend_and_evaluate(model_normalized, t, True)\n",
    "        print(\"--------------------------------------------------------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-14T23:42:09.720136300Z",
     "start_time": "2023-05-14T19:16:09.830160400Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
