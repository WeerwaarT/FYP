{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-14T15:24:16.978966200Z",
     "start_time": "2023-05-14T15:24:10.808345900Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "thresholds = [10, 20, 50, 100, 200]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-14T15:24:16.994980800Z",
     "start_time": "2023-05-14T15:24:16.980968Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "AFP = {}\n",
    "AFP_normalized = {}\n",
    "for t in thresholds:\n",
    "    df = pd.read_csv(f'../LFM-1b_UGP/AFP/LFM-1b_AFP_{t}.txt', sep='\\t')\n",
    "    df.set_index('artist_id', inplace=True)\n",
    "    df = df.loc[(df.sum(axis=1) != 0)]\n",
    "    index_to_artist_id = {index: artist_id for index, artist_id in enumerate(df.index.tolist())}\n",
    "    artist_id_to_index = {artist_id: index for index, artist_id in enumerate(df.index.tolist())}\n",
    "    AFP[t] = (df, index_to_artist_id, artist_id_to_index)\n",
    "\n",
    "    df = pd.read_csv(f'../LFM-1b_UGP/AFP/LFM-1b_AFP_{t}_normalized.txt', sep='\\t')\n",
    "    df.set_index('artist_id', inplace=True)\n",
    "    df = df.loc[(df.sum(axis=1) != 0)]\n",
    "    index_to_artist_id = {index: artist_id for index, artist_id in enumerate(df.index.tolist())}\n",
    "    artist_id_to_index = {artist_id: index for index, artist_id in enumerate(df.index.tolist())}\n",
    "    AFP_normalized[t] = (df, index_to_artist_id, artist_id_to_index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-14T15:24:19.071375200Z",
     "start_time": "2023-05-14T15:24:16.994980800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import heapq\n",
    "import skfuzzy as fuzz\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-14T15:24:28.754697700Z",
     "start_time": "2023-05-14T15:24:19.073376900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "df_plays_test = pd.read_csv('../testing/test_playcounts.txt', sep='\\t')\n",
    "user_ids = df_plays_test['user_id'].unique()\n",
    "liked_artists = {}\n",
    "for t in thresholds:\n",
    "    mask = df_plays_test['artist_id'].isin(AFP[t][2]) & (df_plays_test['playcount'] >= t)\n",
    "    liked_artists[t] = df_plays_test[mask].groupby('user_id')['artist_id'].apply(list).to_dict(into=defaultdict(list))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-14T15:24:28.866799700Z",
     "start_time": "2023-05-14T15:24:28.755698600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def train_and_test(threshold: int, normalized: bool, n_clusters: int, m_value: float = 2.0):\n",
    "    if normalized:\n",
    "        data = AFP[threshold]\n",
    "    else:\n",
    "        data = AFP_normalized[threshold]\n",
    "\n",
    "    _, u, _, _, jm, _, fpc = fuzz.cmeans(data[0].values.T, n_clusters, m_value, error=0.005, maxiter=2000)\n",
    "    if len(jm) == 2000 and abs(jm[-2] - jm[-1]) > 0.005:\n",
    "        print(f\"The algorithm didn't converge.  {m_value} {n_clusters}\")\n",
    "\n",
    "    return u.T"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-14T15:24:28.882814300Z",
     "start_time": "2023-05-14T15:24:28.867800800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def recommend_and_evaluate(fuzzy_c_partitioned_matrix, threshold: int, normalized: bool):\n",
    "    if normalized:\n",
    "        data = AFP[threshold]\n",
    "    else:\n",
    "        data = AFP_normalized[threshold]\n",
    "\n",
    "    precisions = []\n",
    "    distances = []\n",
    "    for i, user_id in enumerate(user_ids):\n",
    "        # print(f\"User {i}\")\n",
    "        artist_ids = liked_artists[threshold][user_id]\n",
    "        if len(artist_ids) < 2:\n",
    "            continue\n",
    "\n",
    "        if len(artist_ids) < 5:\n",
    "            train_artists, test_artists = train_test_split(artist_ids, train_size=0.75, random_state=42)\n",
    "        else:\n",
    "            train_artists, test_artists = train_test_split(artist_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "        k = 100\n",
    "        top_k = []\n",
    "        for artist_id in train_artists:\n",
    "            index = data[2][artist_id]\n",
    "            pw_distances = pairwise_distances(fuzzy_c_partitioned_matrix[[index]], fuzzy_c_partitioned_matrix)\n",
    "            pw_distances[0][index] = float('inf')\n",
    "            for idx, distance in enumerate(pw_distances[0]):\n",
    "                # if sim_value > 0.8:\n",
    "                #     continue\n",
    "\n",
    "                if len(top_k) < k:\n",
    "                    heapq.heappush(top_k, (distance, idx))\n",
    "                else:\n",
    "                    if distance < top_k[0][0]:\n",
    "                        heapq.heapreplace(top_k, (distance, idx))\n",
    "\n",
    "        positive = 0\n",
    "        for distance, index in top_k:\n",
    "            artist_id = data[1][index]\n",
    "            if artist_id in test_artists:\n",
    "                # print(f\"GOAL!       {distance}\")\n",
    "                positive += 1\n",
    "                distances.append(distance)\n",
    "\n",
    "        precisions.append(positive / min(k, len(test_artists)))\n",
    "\n",
    "    print(f'precision: {sum(precisions) / len(precisions)}')\n",
    "    # print('distances')\n",
    "    # print(distances)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-14T16:04:39.081282Z",
     "start_time": "2023-05-14T16:04:39.063265600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "models = defaultdict(dict)\n",
    "models_normalized = defaultdict(dict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-14T16:09:57.188984800Z",
     "start_time": "2023-05-14T16:09:57.176974Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold:  10 n_clusters: 10\n",
      "precision: 0.04435110306947083\n",
      "threshold:  10 n_clusters: 10    normalized\n",
      "precision: 0.044980967333148984\n",
      "--------------------------------------------------------------\n",
      "threshold:  20 n_clusters: 10\n",
      "precision: 0.049862791836809736\n",
      "threshold:  20 n_clusters: 10    normalized\n",
      "precision: 0.05163161102294066\n",
      "--------------------------------------------------------------\n",
      "threshold:  50 n_clusters: 10\n",
      "precision: 0.06531858844202586\n",
      "threshold:  50 n_clusters: 10    normalized\n",
      "precision: 0.06492317523522476\n",
      "--------------------------------------------------------------\n",
      "threshold:  100 n_clusters: 10\n",
      "precision: 0.06510160992539844\n",
      "threshold:  100 n_clusters: 10    normalized\n",
      "precision: 0.06373107639627461\n",
      "--------------------------------------------------------------\n",
      "threshold:  200 n_clusters: 10\n",
      "precision: 0.13232620932988579\n",
      "threshold:  200 n_clusters: 10    normalized\n",
      "precision: 0.13180537599655245\n",
      "--------------------------------------------------------------\n",
      "threshold:  10 n_clusters: 15\n",
      "precision: 0.04362203535203717\n",
      "threshold:  10 n_clusters: 15    normalized\n",
      "precision: 0.04496870550540059\n",
      "--------------------------------------------------------------\n",
      "threshold:  20 n_clusters: 15\n",
      "precision: 0.050057155296479305\n",
      "threshold:  20 n_clusters: 15    normalized\n",
      "precision: 0.05163161102294066\n",
      "--------------------------------------------------------------\n",
      "threshold:  50 n_clusters: 15\n",
      "precision: 0.06531858844202586\n",
      "threshold:  50 n_clusters: 15    normalized\n",
      "precision: 0.06735496645705155\n",
      "--------------------------------------------------------------\n",
      "threshold:  100 n_clusters: 15\n",
      "precision: 0.06482627952892268\n",
      "threshold:  100 n_clusters: 15    normalized\n",
      "precision: 0.06373107639627461\n",
      "--------------------------------------------------------------\n",
      "threshold:  200 n_clusters: 15\n",
      "precision: 0.13024287599655246\n",
      "threshold:  200 n_clusters: 15    normalized\n",
      "precision: 0.13180537599655245\n",
      "--------------------------------------------------------------\n",
      "threshold:  10 n_clusters: 20\n",
      "precision: 0.04509349017193984\n",
      "threshold:  10 n_clusters: 20    normalized\n",
      "precision: 0.0448686234381054\n",
      "--------------------------------------------------------------\n",
      "threshold:  20 n_clusters: 20\n",
      "precision: 0.04987493955303909\n",
      "threshold:  20 n_clusters: 20    normalized\n",
      "precision: 0.049736567291162256\n",
      "--------------------------------------------------------------\n",
      "threshold:  50 n_clusters: 20\n",
      "precision: 0.06531858844202586\n",
      "threshold:  50 n_clusters: 20    normalized\n",
      "precision: 0.06557560702644658\n",
      "--------------------------------------------------------------\n",
      "threshold:  100 n_clusters: 20\n",
      "precision: 0.06240949049319092\n",
      "threshold:  100 n_clusters: 20    normalized\n",
      "precision: 0.06422055265667599\n",
      "--------------------------------------------------------------\n",
      "threshold:  200 n_clusters: 20\n",
      "precision: 0.13024287599655246\n",
      "threshold:  200 n_clusters: 20    normalized\n",
      "precision: 0.13180537599655245\n",
      "--------------------------------------------------------------\n",
      "threshold:  10 n_clusters: 25\n",
      "precision: 0.044855148575438306\n",
      "threshold:  10 n_clusters: 25    normalized\n",
      "precision: 0.044926756213364095\n",
      "--------------------------------------------------------------\n",
      "threshold:  20 n_clusters: 25\n",
      "precision: 0.050069303012708656\n",
      "threshold:  20 n_clusters: 25    normalized\n",
      "precision: 0.04993093075083184\n",
      "--------------------------------------------------------------\n",
      "threshold:  50 n_clusters: 25\n",
      "precision: 0.0650448408373174\n",
      "threshold:  50 n_clusters: 25    normalized\n",
      "precision: 0.06468592731114407\n",
      "--------------------------------------------------------------\n",
      "threshold:  100 n_clusters: 25\n",
      "precision: 0.0657309365459145\n",
      "threshold:  100 n_clusters: 25    normalized\n",
      "precision: 0.06484987927719203\n",
      "--------------------------------------------------------------\n",
      "threshold:  200 n_clusters: 25\n",
      "precision: 0.13232620932988579\n",
      "threshold:  200 n_clusters: 25    normalized\n",
      "precision: 0.13024287599655246\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for n_clusters in [10, 15, 20, 25]:\n",
    "    for t in thresholds:\n",
    "        print(f\"threshold:  {t} n_clusters: {n_clusters}\")\n",
    "        if t in models and n_clusters in models[t]:\n",
    "            model = models[t][n_clusters]\n",
    "            print(\"Found\")\n",
    "        else:\n",
    "            model = train_and_test(t, False, n_clusters)\n",
    "            models[t][n_clusters] = model\n",
    "\n",
    "        recommend_and_evaluate(model, t, False)\n",
    "\n",
    "        print(f\"threshold:  {t} n_clusters: {n_clusters}    normalized\")\n",
    "        if t in models_normalized and n_clusters in models_normalized[t]:\n",
    "            model_normalized = models_normalized[t][n_clusters]\n",
    "            print(\"Found\")\n",
    "        else:\n",
    "            model_normalized = train_and_test(t, True, n_clusters)\n",
    "            models_normalized[t][n_clusters] = model_normalized\n",
    "\n",
    "        recommend_and_evaluate(model_normalized, t, True)\n",
    "        print(\"--------------------------------------------------------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-14T17:12:33.951389400Z",
     "start_time": "2023-05-14T16:09:57.790532900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
